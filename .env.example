# .env.example (Local LLM + Freshdesk)
PORT=3000
BASE_URL=http://localhost:3000
CORS_ORIGINS=*

# KB
KB_ROOT=https://boxful.freshdesk.com/support/solutions/
KB_MAX_PAGES=120
DATA_DIR=./data
VECTORS_FILE=./data/vectors.json

# Chat model
LLM_PROVIDER=ollama
# LLM_PROVIDER=gemini

# Embeddings
EMBED_PROVIDER=ollama
# EMBED_PROVIDER=gemini
EMBED_LOCAL_DIM=768

# Ollama local (CUDA si tu instalación tiene soporte GPU)
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_CHAT_MODEL=llama3.1:8b-instruct-q4_K_M
OLLAMA_EMBED_MODEL=nomic-embed-text:latest
OLLAMA_EMBED_MAX_CHARS=900
# Modelos visibles en el selector de la UI (ideal 2B-4B para pruebas rápidas)
UI_CHAT_MODELS=qwen2.5:1.5b-instruct,llama3.2:3b-instruct,phi3:3.8b-mini-instruct
# Proveedores habilitados en la UI (selector)
UI_LLM_PROVIDERS=ollama,gemini
# Opcional: modelos por proveedor en la UI
# UI_CHAT_MODELS_OLLAMA=qwen2.5:1.5b-instruct,llama3.2:3b-instruct
# UI_CHAT_MODELS_GEMINI=gemini-2.5-flash,gemini-2.5-pro

# Gemini API
# GEMINI_API_KEY=tu_api_key
# GEMINI_CHAT_MODEL=gemini-2.5-flash
# GEMINI_CHAT_MODELS=gemini-2.5-flash,gemini-2.5-pro
# GEMINI_EMBED_MODEL=gemini-embedding-001
# GEMINI_EMBED_MAX_CHARS=8000

# Retrieval
TOP_K=6
MIN_SCORE=0.78

# Ingest
INGEST_CONCURRENCY=1
CHUNK_SIZE_CHARS=950
CHUNK_OVERLAP_CHARS=200
USER_AGENT=BoxfulRAGBot/0.1 (+https://www.boxful.io)
SCRAPE_RETRIES=5
SCRAPE_MIN_DELAY_MS=250
SCRAPE_MAX_DELAY_MS=2500

# Debug
# NO_LLM=1
